{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73badd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/06 16:01:16 WARN Utils: Your hostname, me-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.42.129 instead (on interface ens33)\n",
      "23/12/06 16:01:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/06 16:01:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/06 16:01:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/12/06 16:01:20 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .appName('Data ETL') \\\n",
    "                    .getOrCreate()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ab9a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+--------------+-------+-----------+------------+-------------------+\n",
      "|address_id|             address|address2|      district|city_id|postal_code|       phone|        last_update|\n",
      "+----------+--------------------+--------+--------------+-------+-----------+------------+-------------------+\n",
      "|         1|   47 MySakila Drive|      \\N|       Alberta|    300|       NULL|        NULL|2006-02-15 09:45:30|\n",
      "|         2|  28 MySQL Boulevard|      \\N|           QLD|    576|       NULL|        NULL|2006-02-15 09:45:30|\n",
      "|         3|   23 Workhaven Lane|      \\N|       Alberta|    300|       NULL| 14033335568|2006-02-15 09:45:30|\n",
      "|         4|1411 Lillydale Drive|      \\N|           QLD|    576|       NULL|  6172235589|2006-02-15 09:45:30|\n",
      "|         5|      1913 Hanoi Way|    NULL|      Nagasaki|    463|      35200| 28303384290|2006-02-15 09:45:30|\n",
      "|         6|    1121 Loja Avenue|    NULL|    California|    449|      17886|838635286649|2006-02-15 09:45:30|\n",
      "|         7|   692 Joliet Street|    NULL|        Attika|     38|      83579|448477190408|2006-02-15 09:45:30|\n",
      "|         8|    1566 Inegl Manor|    NULL|      Mandalay|    349|      53561|705814003527|2006-02-15 09:45:30|\n",
      "|         9|     53 Idfu Parkway|    NULL|        Nantou|    361|      42399| 10655648674|2006-02-15 09:45:30|\n",
      "|        10|1795 Santiago de ...|    NULL|         Texas|    295|      18743|860452626434|2006-02-15 09:45:30|\n",
      "|        11|900 Santiago de C...|    NULL|Central Serbia|    280|      93896|716571220373|2006-02-15 09:45:30|\n",
      "|        12|      478 Joliet Way|    NULL|      Hamilton|    200|      77948|657282285970|2006-02-15 09:45:30|\n",
      "|        13|   613 Korolev Drive|    NULL|        Masqat|    329|      45844|380657522649|2006-02-15 09:45:30|\n",
      "|        14|      1531 Sal Drive|    NULL|       Esfahan|    162|      53628|648856936185|2006-02-15 09:45:30|\n",
      "|        15| 1542 Tarlac Parkway|    NULL|      Kanagawa|    440|       1027|635297277345|2006-02-15 09:45:30|\n",
      "|        16|    808 Bhopal Manor|    NULL|       Haryana|    582|      10672|465887807014|2006-02-15 09:45:30|\n",
      "|        17|  270 Amroha Parkway|    NULL|      Osmaniye|    384|      29610|695479687538|2006-02-15 09:45:30|\n",
      "|        18|770 Bydgoszcz Avenue|    NULL|    California|    120|      16266|517338314235|2006-02-15 09:45:30|\n",
      "|        19|     419 Iligan Lane|    NULL|Madhya Pradesh|     76|      72878|990911107354|2006-02-15 09:45:30|\n",
      "|        20|360 Toulouse Parkway|    NULL|       England|    495|      54308|949312333307|2006-02-15 09:45:30|\n",
      "+----------+--------------------+--------+--------------+-------+-----------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_add = spark.read.csv('address.dat',\n",
    "                    sep='\\t',\n",
    "                    schema='address_id int, address string, address2 string, district string, city_id int, postal_code int, phone string, last_update timestamp')\n",
    "\n",
    "df_add.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78bbc26c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-------------------+\n",
      "|category_id|       name|        last_update|\n",
      "+-----------+-----------+-------------------+\n",
      "|          1|     Action|2006-02-15 09:46:27|\n",
      "|          2|  Animation|2006-02-15 09:46:27|\n",
      "|          3|   Children|2006-02-15 09:46:27|\n",
      "|          4|   Classics|2006-02-15 09:46:27|\n",
      "|          5|     Comedy|2006-02-15 09:46:27|\n",
      "|          6|Documentary|2006-02-15 09:46:27|\n",
      "|          7|      Drama|2006-02-15 09:46:27|\n",
      "|          8|     Family|2006-02-15 09:46:27|\n",
      "|          9|    Foreign|2006-02-15 09:46:27|\n",
      "|         10|      Games|2006-02-15 09:46:27|\n",
      "|         11|     Horror|2006-02-15 09:46:27|\n",
      "|         12|      Music|2006-02-15 09:46:27|\n",
      "|         13|        New|2006-02-15 09:46:27|\n",
      "|         14|     Sci-Fi|2006-02-15 09:46:27|\n",
      "|         15|     Sports|2006-02-15 09:46:27|\n",
      "|         16|     Travel|2006-02-15 09:46:27|\n",
      "|       NULL|       NULL|               NULL|\n",
      "+-----------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cat = spark.read.csv('category.dat',\n",
    "                    sep='\\t',\n",
    "                    schema='category_id int, name string, last_update timestamp')\n",
    "\n",
    "df_cat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3d4ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------------+\n",
      "|country_id|       country|        last_update|\n",
      "+----------+--------------+-------------------+\n",
      "|         1|   Afghanistan|2006-02-15 09:44:00|\n",
      "|         2|       Algeria|2006-02-15 09:44:00|\n",
      "|         3|American Samoa|2006-02-15 09:44:00|\n",
      "|         4|        Angola|2006-02-15 09:44:00|\n",
      "|         5|      Anguilla|2006-02-15 09:44:00|\n",
      "|         6|     Argentina|2006-02-15 09:44:00|\n",
      "|         7|       Armenia|2006-02-15 09:44:00|\n",
      "|         8|     Australia|2006-02-15 09:44:00|\n",
      "|         9|       Austria|2006-02-15 09:44:00|\n",
      "|        10|    Azerbaijan|2006-02-15 09:44:00|\n",
      "|        11|       Bahrain|2006-02-15 09:44:00|\n",
      "|        12|    Bangladesh|2006-02-15 09:44:00|\n",
      "|        13|       Belarus|2006-02-15 09:44:00|\n",
      "|        14|       Bolivia|2006-02-15 09:44:00|\n",
      "|        15|        Brazil|2006-02-15 09:44:00|\n",
      "|        16|        Brunei|2006-02-15 09:44:00|\n",
      "|        17|      Bulgaria|2006-02-15 09:44:00|\n",
      "|        18|      Cambodia|2006-02-15 09:44:00|\n",
      "|        19|      Cameroon|2006-02-15 09:44:00|\n",
      "|        20|        Canada|2006-02-15 09:44:00|\n",
      "+----------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_co = spark.read.csv('country.dat',\n",
    "                    sep='\\t',\n",
    "                    schema='country_id int, country string, last_update timestamp')\n",
    "\n",
    "df_co.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d91cc527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+-------------------+\n",
      "|city_id|                city|country_id|        last_update|\n",
      "+-------+--------------------+----------+-------------------+\n",
      "|      1|  A Corua (La Corua)|        87|2006-02-15 09:45:25|\n",
      "|      2|                Abha|        82|2006-02-15 09:45:25|\n",
      "|      3|           Abu Dhabi|       101|2006-02-15 09:45:25|\n",
      "|      4|                Acua|        60|2006-02-15 09:45:25|\n",
      "|      5|               Adana|        97|2006-02-15 09:45:25|\n",
      "|      6|         Addis Abeba|        31|2006-02-15 09:45:25|\n",
      "|      7|                Aden|       107|2006-02-15 09:45:25|\n",
      "|      8|               Adoni|        44|2006-02-15 09:45:25|\n",
      "|      9|          Ahmadnagar|        44|2006-02-15 09:45:25|\n",
      "|     10|            Akishima|        50|2006-02-15 09:45:25|\n",
      "|     11|               Akron|       103|2006-02-15 09:45:25|\n",
      "|     12|              al-Ayn|       101|2006-02-15 09:45:25|\n",
      "|     13|           al-Hawiya|        82|2006-02-15 09:45:25|\n",
      "|     14|           al-Manama|        11|2006-02-15 09:45:25|\n",
      "|     15|          al-Qadarif|        89|2006-02-15 09:45:25|\n",
      "|     16|            al-Qatif|        82|2006-02-15 09:45:25|\n",
      "|     17|         Alessandria|        49|2006-02-15 09:45:25|\n",
      "|     18|Allappuzha (Allep...|        44|2006-02-15 09:45:25|\n",
      "|     19|             Allende|        60|2006-02-15 09:45:25|\n",
      "|     20|     Almirante Brown|         6|2006-02-15 09:45:25|\n",
      "+-------+--------------------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cy = spark.read.csv('city.dat',\n",
    "                    sep='\\t',\n",
    "                    schema='city_id int, city string, country_id int, last_update timestamp')\n",
    "\n",
    "df_cy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e3ad4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+---------+--------------------+----------+----------+-----------+--------------------+------+\n",
      "|customer_id|store_id|first_name|last_name|               email|address_id|activebool|create_date|         last_update|active|\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-----------+--------------------+------+\n",
      "|        524|       1|     Jared|      Ely|jared.ely@sakilac...|       530|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          1|       1|      Mary|    Smith|mary.smith@sakila...|         5|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          2|       1|  Patricia|  Johnson|patricia.johnson@...|         6|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          3|       1|     Linda| Williams|linda.williams@sa...|         7|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          4|       2|   Barbara|    Jones|barbara.jones@sak...|         8|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          5|       1| Elizabeth|    Brown|elizabeth.brown@s...|         9|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          6|       2|  Jennifer|    Davis|jennifer.davis@sa...|        10|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          7|       1|     Maria|   Miller|maria.miller@saki...|        11|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          8|       2|     Susan|   Wilson|susan.wilson@saki...|        12|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|          9|       2|  Margaret|    Moore|margaret.moore@sa...|        13|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         10|       1|   Dorothy|   Taylor|dorothy.taylor@sa...|        14|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         11|       2|      Lisa| Anderson|lisa.anderson@sak...|        15|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         12|       1|     Nancy|   Thomas|nancy.thomas@saki...|        16|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         13|       2|     Karen|  Jackson|karen.jackson@sak...|        17|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         14|       2|     Betty|    White|betty.white@sakil...|        18|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         15|       1|     Helen|   Harris|helen.harris@saki...|        19|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         16|       2|    Sandra|   Martin|sandra.martin@sak...|        20|         t| 2006-02-14|2013-05-26 14:49:...|     0|\n",
      "|         17|       1|     Donna| Thompson|donna.thompson@sa...|        21|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         18|       2|     Carol|   Garcia|carol.garcia@saki...|        22|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "|         19|       1|      Ruth| Martinez|ruth.martinez@sak...|        23|         t| 2006-02-14|2013-05-26 14:49:...|     1|\n",
      "+-----------+--------+----------+---------+--------------------+----------+----------+-----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cs = spark.read.csv('customer.dat',\n",
    "                    sep='\\t',\n",
    "                    schema=\"\"\"customer_id int, store_id int, first_name string, last_name string, \n",
    "                    email string, address_id int, activebool string, create_date date, last_update timestamp, active byte\"\"\")\n",
    "\n",
    "df_cs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd93b947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+------------+-----------+---------------+-----------+------+----------------+------+-----------+\n",
      "|film_id|            title|         description|release_year|language_id|rental_duration|rental_rate|length|replacement_cost|rating|last_update|\n",
      "+-------+-----------------+--------------------+------------+-----------+---------------+-----------+------+----------------+------+-----------+\n",
      "|    133|  Chamber Italian|A Fateful Reflect...|        2006|          1|              7|       4.99|   117|           14.99| NC-17| 2013-05-26|\n",
      "|    384| Grosse Wonderful|A Epic Drama of a...|        2006|          1|              5|       4.99|    49|           19.99|     R| 2013-05-26|\n",
      "|      8|  Airport Pollock|A Epic Tale of a ...|        2006|          1|              6|       4.99|    54|           15.99|     R| 2013-05-26|\n",
      "|     98|Bright Encounters|A Fateful Yarn of...|        2006|          1|              4|       4.99|    73|           12.99| PG-13| 2013-05-26|\n",
      "|      1| Academy Dinosaur|A Epic Drama of a...|        2006|          1|              6|       0.99|    86|           20.99|    PG| 2013-05-26|\n",
      "|      2|   Ace Goldfinger|A Astounding Epis...|        2006|          1|              3|       4.99|    48|           12.99|     G| 2013-05-26|\n",
      "|      3| Adaptation Holes|A Astounding Refl...|        2006|          1|              7|       2.99|    50|           18.99| NC-17| 2013-05-26|\n",
      "|      4| Affair Prejudice|A Fanciful Docume...|        2006|          1|              5|       2.99|   117|           26.99|     G| 2013-05-26|\n",
      "|      5|      African Egg|A Fast-Paced Docu...|        2006|          1|              6|       2.99|   130|           22.99|     G| 2013-05-26|\n",
      "|      6|     Agent Truman|A Intrepid Panora...|        2006|          1|              3|       2.99|   169|           17.99|    PG| 2013-05-26|\n",
      "|      7|  Airplane Sierra|A Touching Saga o...|        2006|          1|              6|       4.99|    62|           28.99| PG-13| 2013-05-26|\n",
      "|      9|    Alabama Devil|A Thoughtful Pano...|        2006|          1|              3|       2.99|   114|           21.99| PG-13| 2013-05-26|\n",
      "|     10| Aladdin Calendar|A Action-Packed T...|        2006|          1|              6|       4.99|    63|           24.99| NC-17| 2013-05-26|\n",
      "|     11|  Alamo Videotape|A Boring Epistle ...|        2006|          1|              6|       0.99|   126|           16.99|     G| 2013-05-26|\n",
      "|     12|   Alaska Phantom|A Fanciful Saga o...|        2006|          1|              6|       0.99|   136|           22.99|    PG| 2013-05-26|\n",
      "|    213|       Date Speed|A Touching Saga o...|        2006|          1|              4|       0.99|   104|           19.99|     R| 2013-05-26|\n",
      "|     13|      Ali Forever|A Action-Packed D...|        2006|          1|              4|       4.99|   150|           21.99|    PG| 2013-05-26|\n",
      "|     14|   Alice Fantasia|A Emotional Drama...|        2006|          1|              6|       0.99|    94|           23.99| NC-17| 2013-05-26|\n",
      "|     15|     Alien Center|A Brilliant Drama...|        2006|          1|              5|       2.99|    46|           10.99| NC-17| 2013-05-26|\n",
      "|     16|  Alley Evolution|A Fast-Paced Dram...|        2006|          1|              6|       2.99|   180|           23.99| NC-17| 2013-05-26|\n",
      "+-------+-----------------+--------------------+------------+-----------+---------------+-----------+------+----------------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fm = spark.read.csv('film.dat',\n",
    "                    sep='\\t',\n",
    "                    schema=\"\"\"film_id int, title string, description string, release_year int, language_id int, rental_duration int, rental_rate float, \n",
    "                            length int, replacement_cost float, rating string, last_update date\"\"\")\n",
    "\n",
    "df_fm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec2ba14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------------+\n",
      "|film_id|category_id|        last_update|\n",
      "+-------+-----------+-------------------+\n",
      "|      1|          6|2006-02-15 10:07:09|\n",
      "|      2|         11|2006-02-15 10:07:09|\n",
      "|      3|          6|2006-02-15 10:07:09|\n",
      "|      4|         11|2006-02-15 10:07:09|\n",
      "|      5|          8|2006-02-15 10:07:09|\n",
      "|      6|          9|2006-02-15 10:07:09|\n",
      "|      7|          5|2006-02-15 10:07:09|\n",
      "|      8|         11|2006-02-15 10:07:09|\n",
      "|      9|         11|2006-02-15 10:07:09|\n",
      "|     10|         15|2006-02-15 10:07:09|\n",
      "|     11|          9|2006-02-15 10:07:09|\n",
      "|     12|         12|2006-02-15 10:07:09|\n",
      "|     13|         11|2006-02-15 10:07:09|\n",
      "|     14|          4|2006-02-15 10:07:09|\n",
      "|     15|          9|2006-02-15 10:07:09|\n",
      "|     16|          9|2006-02-15 10:07:09|\n",
      "|     17|         12|2006-02-15 10:07:09|\n",
      "|     18|          2|2006-02-15 10:07:09|\n",
      "|     19|          1|2006-02-15 10:07:09|\n",
      "|     20|         12|2006-02-15 10:07:09|\n",
      "+-------+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fc = spark.read.csv('film_category.dat',\n",
    "                    sep='\\t',\n",
    "                    schema='film_id int, category_id int, last_update timestamp')\n",
    "\n",
    "df_fc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69802f58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+--------+-----------+\n",
      "|inventory_id|film_id|store_id|last_update|\n",
      "+------------+-------+--------+-----------+\n",
      "|           1|      1|       1| 2006-02-15|\n",
      "|           2|      1|       1| 2006-02-15|\n",
      "|           3|      1|       1| 2006-02-15|\n",
      "|           4|      1|       1| 2006-02-15|\n",
      "|           5|      1|       2| 2006-02-15|\n",
      "|           6|      1|       2| 2006-02-15|\n",
      "|           7|      1|       2| 2006-02-15|\n",
      "|           8|      1|       2| 2006-02-15|\n",
      "|           9|      2|       2| 2006-02-15|\n",
      "|          10|      2|       2| 2006-02-15|\n",
      "|          11|      2|       2| 2006-02-15|\n",
      "|          12|      3|       2| 2006-02-15|\n",
      "|          13|      3|       2| 2006-02-15|\n",
      "|          14|      3|       2| 2006-02-15|\n",
      "|          15|      3|       2| 2006-02-15|\n",
      "|          16|      4|       1| 2006-02-15|\n",
      "|          17|      4|       1| 2006-02-15|\n",
      "|          18|      4|       1| 2006-02-15|\n",
      "|          19|      4|       1| 2006-02-15|\n",
      "|          20|      4|       2| 2006-02-15|\n",
      "+------------+-------+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iv = spark.read.csv('inventory.dat',\n",
    "                    sep='\\t',\n",
    "                    schema='inventory_id int, film_id int, store_id int, last_update date')\n",
    "\n",
    "df_iv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df2b7ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------------+\n",
      "|language_id|                name|        last_update|\n",
      "+-----------+--------------------+-------------------+\n",
      "|          1|English             |2006-02-15 10:02:19|\n",
      "|          2|Italian             |2006-02-15 10:02:19|\n",
      "|          3|Japanese            |2006-02-15 10:02:19|\n",
      "|          4|Mandarin            |2006-02-15 10:02:19|\n",
      "|          5|French              |2006-02-15 10:02:19|\n",
      "|          6|German              |2006-02-15 10:02:19|\n",
      "|       NULL|                NULL|               NULL|\n",
      "+-----------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lg = spark.read.csv('language.dat',\n",
    "                    sep='\\t',\n",
    "                    schema='language_id int, name string, last_update timestamp')\n",
    "\n",
    "df_lg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1d1228c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|        payment_date|\n",
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "|     17503|        341|       2|     1520|  7.99|2007-02-15 22:25:...|\n",
      "|     17504|        341|       1|     1778|  1.99|2007-02-16 17:23:...|\n",
      "|     17505|        341|       1|     1849|  7.99|2007-02-16 22:41:...|\n",
      "|     17506|        341|       2|     2829|  2.99|2007-02-19 19:39:...|\n",
      "|     17507|        341|       2|     3130|  7.99|2007-02-20 17:31:...|\n",
      "|     17508|        341|       1|     3382|  5.99|2007-02-21 12:33:...|\n",
      "|     17509|        342|       2|     2190|  5.99|2007-02-17 23:58:...|\n",
      "|     17510|        342|       1|     2914|  5.99|2007-02-20 02:11:...|\n",
      "|     17511|        342|       1|     3081|  2.99|2007-02-20 13:57:...|\n",
      "|     17512|        343|       2|     1547|  4.99|2007-02-16 00:10:...|\n",
      "|     17513|        343|       1|     1564|  6.99|2007-02-16 01:15:...|\n",
      "|     17514|        343|       2|     1879|  0.99|2007-02-17 01:26:...|\n",
      "|     17515|        343|       2|     1922|  0.99|2007-02-17 04:32:...|\n",
      "|     17516|        343|       2|     2461|  6.99|2007-02-18 18:26:...|\n",
      "|     17517|        343|       1|     2980|  8.99|2007-02-20 07:03:...|\n",
      "|     17518|        343|       1|     3407|  0.99|2007-02-21 14:42:...|\n",
      "|     17519|        344|       1|     1341|  3.99|2007-02-15 10:54:...|\n",
      "|     17520|        344|       2|     1475|  4.99|2007-02-15 19:36:...|\n",
      "|     17521|        344|       1|     1731|  0.99|2007-02-16 14:00:...|\n",
      "|     17522|        345|       2|     1210|  0.99|2007-02-15 01:26:...|\n",
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_py = spark.read.csv('payment.dat',\n",
    "                    sep='\\t',\n",
    "                    schema='payment_id int, customer_id int, staff_id int, rental_id int, amount float, payment_date timestamp')\n",
    "\n",
    "df_py.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ee510ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "|rental_id|        rental_date|inventory_id|customer_id|        return_date|staff_id|        last_update|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "|        2|2005-05-24 22:54:33|        1525|        459|2005-05-28 19:40:33|       1|2006-02-16 02:30:53|\n",
      "|        3|2005-05-24 23:03:39|        1711|        408|2005-06-01 22:12:39|       1|2006-02-16 02:30:53|\n",
      "|        4|2005-05-24 23:04:41|        2452|        333|2005-06-03 01:43:41|       2|2006-02-16 02:30:53|\n",
      "|        5|2005-05-24 23:05:21|        2079|        222|2005-06-02 04:33:21|       1|2006-02-16 02:30:53|\n",
      "|        6|2005-05-24 23:08:07|        2792|        549|2005-05-27 01:32:07|       1|2006-02-16 02:30:53|\n",
      "|        7|2005-05-24 23:11:53|        3995|        269|2005-05-29 20:34:53|       2|2006-02-16 02:30:53|\n",
      "|        8|2005-05-24 23:31:46|        2346|        239|2005-05-27 23:33:46|       2|2006-02-16 02:30:53|\n",
      "|        9|2005-05-25 00:00:40|        2580|        126|2005-05-28 00:22:40|       1|2006-02-16 02:30:53|\n",
      "|       10|2005-05-25 00:02:21|        1824|        399|2005-05-31 22:44:21|       2|2006-02-16 02:30:53|\n",
      "|       11|2005-05-25 00:09:02|        4443|        142|2005-06-02 20:56:02|       2|2006-02-16 02:30:53|\n",
      "|       12|2005-05-25 00:19:27|        1584|        261|2005-05-30 05:44:27|       2|2006-02-16 02:30:53|\n",
      "|       13|2005-05-25 00:22:55|        2294|        334|2005-05-30 04:28:55|       1|2006-02-16 02:30:53|\n",
      "|       14|2005-05-25 00:31:15|        2701|        446|2005-05-26 02:56:15|       1|2006-02-16 02:30:53|\n",
      "|       15|2005-05-25 00:39:22|        3049|        319|2005-06-03 03:30:22|       1|2006-02-16 02:30:53|\n",
      "|       16|2005-05-25 00:43:11|         389|        316|2005-05-26 04:42:11|       2|2006-02-16 02:30:53|\n",
      "|       17|2005-05-25 01:06:36|         830|        575|2005-05-27 00:43:36|       1|2006-02-16 02:30:53|\n",
      "|       18|2005-05-25 01:10:47|        3376|         19|2005-05-31 06:35:47|       2|2006-02-16 02:30:53|\n",
      "|       19|2005-05-25 01:17:24|        1941|        456|2005-05-31 06:00:24|       1|2006-02-16 02:30:53|\n",
      "|       20|2005-05-25 01:48:41|        3517|        185|2005-05-27 02:20:41|       2|2006-02-16 02:30:53|\n",
      "|       21|2005-05-25 01:59:46|         146|        388|2005-05-26 01:01:46|       2|2006-02-16 02:30:53|\n",
      "+---------+-------------------+------------+-----------+-------------------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rt = spark.read.csv('rental.dat',\n",
    "                    sep='\\t',\n",
    "                    schema='rental_id int, rental_date timestamp, inventory_id int, customer_id int, return_date timestamp, staff_id int, last_update timestamp')\n",
    "\n",
    "df_rt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dd9b762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+----------+--------------------+--------+\n",
      "|staff_id|first_name|last_name|address_id|               email|store_id|\n",
      "+--------+----------+---------+----------+--------------------+--------+\n",
      "|       1|      Mike|  Hillyer|         3|Mike.Hillyer@saki...|       1|\n",
      "|       2|       Jon| Stephens|         4|Jon.Stephens@saki...|       2|\n",
      "|    NULL|      NULL|     NULL|      NULL|                NULL|    NULL|\n",
      "+--------+----------+---------+----------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sf = spark.read.csv('staff.dat',\n",
    "                    sep='\\t',\n",
    "                    schema='staff_id int, first_name string, last_name string, address_id int, email string, store_id int')\n",
    "\n",
    "df_sf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64ce3117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------+\n",
      "|store_id|manager_staff_id|address_id|\n",
      "+--------+----------------+----------+\n",
      "|       1|               1|         1|\n",
      "|       2|               2|         2|\n",
      "|    NULL|            NULL|      NULL|\n",
      "+--------+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_st = spark.read.csv('store.dat',\n",
    "                    sep='\\t',\n",
    "                    schema='store_id int, manager_staff_id int, address_id int')\n",
    "\n",
    "df_st.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d99747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning dataframes of null rows\n",
    "# Caching dataframes that are used more than once\n",
    "# Repartitioning dataframes with many rows\n",
    "\n",
    "df_add = df_add.na.drop(\"all\").cache()\n",
    "df_cat = df_cat.na.drop(\"all\")\n",
    "df_cy = df_cy.na.drop(\"all\").cache()\n",
    "df_co = df_co.na.drop('all')\n",
    "df_cs = df_cs.na.drop(\"all\")\n",
    "df_fm = df_fm.na.drop(\"all\").repartition(8).drop()\n",
    "df_fc = df_fc.na.drop(\"all\")\n",
    "df_iv = df_iv.na.drop(\"all\").drop().cache()\n",
    "df_lg = df_lg.na.drop(\"all\").cache()\n",
    "df_py = df_py.na.drop(\"all\").repartition(8).cache()\n",
    "df_rt = df_rt.na.drop(\"all\").repartition(8)\n",
    "df_sf = df_sf.na.drop(\"all\")\n",
    "df_st = df_st.na.drop(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d2e44ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.createOrReplaceTempView('address')\n",
    "df_cat.createOrReplaceTempView('category')\n",
    "df_cy.createOrReplaceTempView('city')\n",
    "df_co.createOrReplaceTempView('country')\n",
    "df_cs.createOrReplaceTempView('customer')\n",
    "df_fm.createOrReplaceTempView('film')\n",
    "df_fc.createOrReplaceTempView('film_category')\n",
    "df_iv.createOrReplaceTempView('inventory')\n",
    "df_lg.createOrReplaceTempView('language')\n",
    "df_py.createOrReplaceTempView('payment')\n",
    "df_rt.createOrReplaceTempView('rental')\n",
    "df_sf.createOrReplaceTempView('staff')\n",
    "df_st.createOrReplaceTempView('store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55cfbc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+-----------------+----------+--------------+-------------+\n",
      "|customer_id|first_name|last_name|active|          address|  district|          city|      country|\n",
      "+-----------+----------+---------+------+-----------------+----------+--------------+-------------+\n",
      "|          1|      Mary|    Smith|     1|   1913 Hanoi Way|  Nagasaki|        Sasebo|        Japan|\n",
      "|          2|  Patricia|  Johnson|     1| 1121 Loja Avenue|California|San Bernardino|United States|\n",
      "|          3|     Linda| Williams|     1|692 Joliet Street|    Attika|       Athenai|       Greece|\n",
      "|          4|   Barbara|    Jones|     1| 1566 Inegl Manor|  Mandalay|      Myingyan|      Myanmar|\n",
      "+-----------+----------+---------+------+-----------------+----------+--------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_customer = spark.sql(\n",
    "\"\"\"\n",
    "select\n",
    "c.customer_id,\n",
    "c.first_name,\n",
    "c.last_name,\n",
    "c.active,\n",
    "a.address,\n",
    "a.district,\n",
    "ci.city,\n",
    "co.country\n",
    "\n",
    "from customer as c join address as a\n",
    "    on c.address_id = a.address_id\n",
    "join city as ci\n",
    "    on a.city_id = ci.city_id\n",
    "join country co \n",
    "    on ci.country_id = co.country_id\n",
    "\"\"\"\n",
    ")\n",
    "dim_customer.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb442d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------------+------------+---------------+-----------+------+----------------+------+--------------------+--------+\n",
      "|film_id|          title|         description|release_year|rental_duration|rental_rate|length|replacement_cost|rating|            language|category|\n",
      "+-------+---------------+--------------------+------------+---------------+-----------+------+----------------+------+--------------------+--------+\n",
      "|    646|Outbreak Divine|A Unbelieveable Y...|        2006|              6|       0.99|   169|           12.99| NC-17|English             |   Games|\n",
      "|    283|  Ending Crowds|A Unbelieveable D...|        2006|              6|       0.99|    85|           10.99| NC-17|English             |     New|\n",
      "|    707|Quest Mussolini|A Fateful Drama o...|        2006|              5|       2.99|   177|           29.99|     R|English             |  Action|\n",
      "|    233|Disciple Mother|A Touching Reflec...|        2006|              3|       0.99|   141|           17.99|    PG|English             |  Travel|\n",
      "+-------+---------------+--------------------+------------+---------------+-----------+------+----------------+------+--------------------+--------+\n",
      "only showing top 4 rows\n",
      "\n",
      "root\n",
      " |-- film_id: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- release_year: integer (nullable = true)\n",
      " |-- rental_duration: integer (nullable = true)\n",
      " |-- rental_rate: float (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      " |-- replacement_cost: float (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dim_film = spark.sql(\n",
    "\"\"\"\n",
    "select\n",
    "f.film_id,\n",
    "f.title,\n",
    "f.description,\n",
    "f.release_year,\n",
    "f.rental_duration,\n",
    "f.rental_rate,\n",
    "f.length,\n",
    "f.replacement_cost,\n",
    "f.rating,\n",
    "l.name as language,\n",
    "c.name as category\n",
    "\n",
    "from film as f join language as l\n",
    "    on f.language_id = l.language_id\n",
    "join film_category as fc\n",
    "    on f.film_id = fc.film_id\n",
    "join category as c\n",
    "    on fc.category_id = c.category_id\n",
    "\"\"\"\n",
    ")\n",
    "dim_film.show(4)\n",
    "dim_film.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4b2ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+--------+-----------+----------+---------+\n",
      "|store_id|           address|district|postal_code|      city|  country|\n",
      "+--------+------------------+--------+-----------+----------+---------+\n",
      "|       1| 47 MySakila Drive| Alberta|       NULL|Lethbridge|   Canada|\n",
      "|       2|28 MySQL Boulevard|     QLD|       NULL| Woodridge|Australia|\n",
      "+--------+------------------+--------+-----------+----------+---------+\n",
      "\n",
      "root\n",
      " |-- store_id: integer (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- postal_code: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_store = spark.sql(\n",
    "\"\"\"\n",
    "select\n",
    "s.store_id,\n",
    "a.address,\n",
    "a.district,\n",
    "a.postal_code,\n",
    "c.city,\n",
    "co.country\n",
    "\n",
    "from store as s join address as a\n",
    "    on s.address_id = a.address_id\n",
    "join city as c\n",
    "    on a.city_id = c.city_id\n",
    "join country as co\n",
    "    on c.country_id = co.country_id\n",
    "\"\"\"\n",
    ")\n",
    "dim_store.show(4)\n",
    "dim_store.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce398b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, quarter, dayofweek\n",
    "from pyspark.sql.functions import col, date_format, to_date, unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d202ed5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- staff_id: integer (nullable = true)\n",
      " |-- rental_id: integer (nullable = true)\n",
      " |-- amount: float (nullable = true)\n",
      " |-- payment_date: timestamp (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 34:=============================>                            (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+-----+---+-------+---------+\n",
      "|      date|  date_key|year|month|day|quarter|dayofweek|\n",
      "+----------+----------+----+-----+---+-------+---------+\n",
      "|2007-03-22|1174492800|2007|    3| 22|      1|        5|\n",
      "|2007-03-17|1174060800|2007|    3| 17|      1|        7|\n",
      "|2007-04-27|1177603200|2007|    4| 27|      2|        6|\n",
      "|2007-03-02|1172764800|2007|    3|  2|      1|        6|\n",
      "|2007-04-30|1177862400|2007|    4| 30|      2|        2|\n",
      "|2007-04-26|1177516800|2007|    4| 26|      2|        5|\n",
      "|2007-04-12|1176307200|2007|    4| 12|      2|        5|\n",
      "|2007-02-18|1171728000|2007|    2| 18|      1|        1|\n",
      "|2007-02-14|1171382400|2007|    2| 14|      1|        4|\n",
      "|2007-03-19|1174233600|2007|    3| 19|      1|        2|\n",
      "|2007-02-20|1171900800|2007|    2| 20|      1|        3|\n",
      "|2007-02-19|1171814400|2007|    2| 19|      1|        2|\n",
      "|2007-04-11|1176220800|2007|    4| 11|      2|        4|\n",
      "|2007-04-07|1175875200|2007|    4|  7|      2|        7|\n",
      "|2007-02-17|1171641600|2007|    2| 17|      1|        7|\n",
      "|2007-04-29|1177776000|2007|    4| 29|      2|        1|\n",
      "|2007-03-18|1174147200|2007|    3| 18|      1|        1|\n",
      "|2007-05-14|1179072000|2007|    5| 14|      2|        2|\n",
      "|2007-03-16|1173974400|2007|    3| 16|      1|        6|\n",
      "|2007-04-28|1177689600|2007|    4| 28|      2|        7|\n",
      "+----------+----------+----+-----+---+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- date_key: long (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_py_date = df_py.withColumn(\"date\", to_date(col(\"payment_date\"), \"yyyy-MM-dd\"))\n",
    "df_py_date.printSchema()\n",
    "\n",
    "dim_date = df_py_date.select('date').distinct()\n",
    "dim_date = dim_date.withColumn('date_key', unix_timestamp('date').cast('long'))\\\n",
    "            .withColumn('year', year('date'))\\\n",
    "            .withColumn('month', month('date'))\\\n",
    "            .withColumn('day', dayofmonth('date'))\\\n",
    "            .withColumn('quarter', quarter('date'))\\\n",
    "            .withColumn('dayofweek', dayofweek('date'))\n",
    "dim_date.show()\n",
    "dim_date.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a595969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------+--------+--------------------+-----------+-------------------+-------------------+\n",
      "|payment_id|customer_id|film_id|store_id|        payment_date|sale_amount|        rental_date|        return_date|\n",
      "+----------+-----------+-------+--------+--------------------+-----------+-------------------+-------------------+\n",
      "|     24779|        226|    266|       1|2007-03-21 19:51:...|       0.99|2005-08-21 21:22:56|2005-08-25 01:53:56|\n",
      "|     27681|        490|    489|       2|2007-04-11 12:30:...|       2.99|2005-07-11 14:02:19|2005-07-17 08:11:19|\n",
      "|     27215|        447|     19|       1|2007-04-12 09:25:...|       0.99|2005-07-12 10:57:28|2005-07-15 06:06:28|\n",
      "|     31137|        189|    602|       2|2007-04-07 09:52:...|       0.99|2005-07-07 11:24:14|2005-07-11 16:26:14|\n",
      "+----------+-----------+-------+--------+--------------------+-----------+-------------------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- film_id: integer (nullable = true)\n",
      " |-- store_id: integer (nullable = true)\n",
      " |-- payment_date: timestamp (nullable = true)\n",
      " |-- sale_amount: float (nullable = true)\n",
      " |-- rental_date: timestamp (nullable = true)\n",
      " |-- return_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_sale = spark.sql(\n",
    "\"\"\"\n",
    "select\n",
    "p.payment_id,\n",
    "p.customer_id,\n",
    "i.film_id,\n",
    "s.store_id,\n",
    "p.payment_date,\n",
    "p.amount as sale_amount,\n",
    "r.rental_date,\n",
    "r.return_date\n",
    "\n",
    "from payment as p join rental as r\n",
    "    on p.rental_id = r.rental_id\n",
    "join inventory as i \n",
    "    on r.inventory_id = i.inventory_id\n",
    "join staff as s\n",
    "    on p.staff_id = s.staff_id\n",
    "\"\"\"\n",
    ")\n",
    "fact_sale.show(4)\n",
    "fact_sale.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60ab108c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14596\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(fact_sale.count())\n",
    "print(dim_store.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "432ad253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 51:>                                                        (0 + 8) / 10]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dim_customer.repartition(10).write.csv('./transformed_data/dim_customer', mode='overwrite', header=True)\n",
    "dim_film.repartition(10).write.csv('./transformed_data/dim_film', mode='overwrite', header=True)\n",
    "dim_store.repartition(10).write.csv('./transformed_data/dim_store', mode='overwrite', header=True)\n",
    "dim_date.repartition(10).write.csv('./transformed_data/dim_date', mode='overwrite', header=True)\n",
    "fact_sale.repartition(10).write.csv('./transformed_data/fact_sales', mode='overwrite', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62cc7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dfs = {'dim_customer':dim_customer,\n",
    "                  'dim_film':dim_film,\n",
    "                  'dim_store':dim_store,\n",
    "                  'dim_date':dim_date,\n",
    "                  'fact_sale':fact_sale}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
